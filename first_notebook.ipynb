{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homework #2 Slide 14: Make a Colab code\n",
    "using PyTorch to\n",
    "determine the optimal\n",
    "points for the means\n",
    "shown in the text\n",
    "alongside – you make\n",
    "up the problem and\n",
    "you solve it yourself to\n",
    "fully understand the\n",
    "role of cost functions in\n",
    "optimization problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how different cost (loss) functions lead to different optimal points\n",
    "for the same dataset. We numerically verify that:\n",
    "\n",
    "- L2 loss → arithmetic mean\n",
    "- L1 loss → median\n",
    "- Log-space L2 loss → geometric mean\n",
    "- Reciprocal-space L2 loss → harmonic mean\n",
    "\n",
    "All optimizations are done using gradient descent in PyTorch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: [1.0, 2.0, 4.0, 8.0]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# synthetic dataset\n",
    "x = torch.tensor([1.0, 2.0, 4.0, 8.0])\n",
    "print(\"Data:\", x.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2 Loss → Arithmetic Mean\n",
    "\n",
    "Cost function:\n",
    "J(u) = sum (x_i - u)^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arithmetic mean (gradient descent): 3.75\n",
      "True arithmetic mean: 3.75\n"
     ]
    }
   ],
   "source": [
    "u = torch.tensor(0.0, requires_grad=True)\n",
    "optimizer = torch.optim.SGD([u], lr=0.1)\n",
    "\n",
    "for _ in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    loss = torch.sum((x - u)**2)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(\"Arithmetic mean (gradient descent):\", u.item())\n",
    "print(\"True arithmetic mean:\", x.mean().item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 Loss → Median\n",
    "\n",
    "Cost function:\n",
    "J(u) = sum |x_i - u|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median (gradient descent): 2.0500001907348633\n",
      "True median: 2.0\n"
     ]
    }
   ],
   "source": [
    "u = torch.tensor(0.0, requires_grad=True)\n",
    "optimizer = torch.optim.SGD([u], lr=0.05)\n",
    "\n",
    "for _ in range(300):\n",
    "    optimizer.zero_grad()\n",
    "    loss = torch.sum(torch.abs(x - u))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(\"Median (gradient descent):\", u.item())\n",
    "print(\"True median:\", x.median().item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log-Space L2 Loss → Geometric Mean\n",
    "\n",
    "Cost function:\n",
    "J(u) = sum (log x_i - log u)^2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geometric mean (gradient descent): 2.8284261226654053\n",
      "True geometric mean: 2.8284270763397217\n"
     ]
    }
   ],
   "source": [
    "u = torch.tensor(1.0, requires_grad=True)\n",
    "optimizer = torch.optim.SGD([u], lr=0.1)\n",
    "\n",
    "for _ in range(300):\n",
    "    optimizer.zero_grad()\n",
    "    loss = torch.sum((torch.log(x) - torch.log(u))**2)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(\"Geometric mean (gradient descent):\", u.item())\n",
    "print(\"True geometric mean:\", torch.exp(torch.mean(torch.log(x))).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reciprocal-Space L2 Loss → Harmonic Mean\n",
    "\n",
    "Cost function:\n",
    "J(u) = sum (1/x_i - 1/u)^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harmonic mean (gradient descent): 2.133333921432495\n",
      "True harmonic mean: 2.1333333333333333\n"
     ]
    }
   ],
   "source": [
    "u = torch.tensor(1.0, requires_grad=True)\n",
    "optimizer = torch.optim.SGD([u], lr=0.5)\n",
    "\n",
    "for _ in range(400):\n",
    "    optimizer.zero_grad()\n",
    "    loss = torch.sum((1/x - 1/u)**2)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(\"Harmonic mean (gradient descent):\", u.item())\n",
    "print(\"True harmonic mean:\", len(x) / torch.sum(1/x).item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of Optimal Points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arithmetic Mean (L2): 3.75\n",
      "Median (L1): 2.0\n",
      "Geometric Mean: 2.8284270763397217\n",
      "Harmonic Mean: 2.1333333333333333\n"
     ]
    }
   ],
   "source": [
    "means = {\n",
    "    \"Arithmetic Mean (L2)\": x.mean().item(),\n",
    "    \"Median (L1)\": x.median().item(),\n",
    "    \"Geometric Mean\": torch.exp(torch.mean(torch.log(x))).item(),\n",
    "    \"Harmonic Mean\": len(x) / torch.sum(1/x).item()\n",
    "}\n",
    "\n",
    "for k, v in means.items():\n",
    "    print(f\"{k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This experiment shows that the optimal solution of an optimization problem depends entirely\n",
    "on the chosen cost function. Different losses emphasize different aspects of the data, leading\n",
    "to different “best” points even when the dataset is fixed.\n",
    "\n",
    "Optimization is therefore not only about data — it is about what you choose to penalize, and this speaks to the variability we see with different uses of functions.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
